{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtybilly/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-17 00:06:12,177\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os.path as osp\n",
    "sys.path.append(osp.join(os.getcwd(), \"..\"))\n",
    "from vllm import LLM, SamplingParams\n",
    "from arguments import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "download_dir = '/media/mtybilly/My Passport1/Program/GLLM/models/LLMs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-17 00:06:14 llm_engine.py:87] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir='/media/mtybilly/My Passport1/Program/GLLM/models/LLMs', load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtybilly/anaconda3/envs/llm/lib/python3.11/site-packages/cupy/_environment.py:404: UserWarning: \n",
      "nccl library could not be loaded.\n",
      "\n",
      "Reason: ImportError (libnccl.so.2: cannot open shared object file: No such file or directory)\n",
      "\n",
      "You can install the library by:\n",
      "\n",
      "  $ python -m cupyx.tools.install_library --library nccl --cuda 12.x\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-17 00:06:16 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 04-17 00:09:11 llm_engine.py:357] # GPU blocks: 1039, # CPU blocks: 512\n",
      "INFO 04-17 00:09:11 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-17 00:09:11 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-17 00:09:14 model_runner.py:756] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\n",
    "        model=model_name,\n",
    "        download_dir=download_dir,\n",
    "        dtype=\"half\",\n",
    "        tensor_parallel_size=1,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]<<SYS>>\n",
      "Please answer the question\n",
      "<</SYS>>\n",
      "\n",
      "Question: What is the Transformer?[/INST]\n",
      "\n",
      " Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<s>[INST]<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n\"\\\n",
    "        \"Question: {user_message}[/INST]\\n\\n Answer:\"\n",
    "prompt = prompt.format(system_prompt=\"Please answer the question\", user_message=\"What is the Transformer?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]<<SYS>>\n",
      "INSTRUCTION: Answer the question according to the documents. Each line in documents is a row or column extract from table.\n",
      "<</SYS>>\n",
      "\n",
      "QUESTION: What is the first time this team attend the World Cup?\n",
      "\n",
      "DOCUMENTS:\n",
      "Year is 1998, World Cup is No.\n",
      "Year is 1999, World Cup is No.\n",
      "Year is 2000, World Cup is Yes.\n",
      "Year is 2001, World Cup is Yes.\n",
      "Year is 2002, World Cup is No.\n",
      "Year is 2003, World Cup is No.\n",
      "[/INST]\n",
      "\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<s>[INST]<<SYS>>\\n\"\\\n",
    "        \"INSTRUCTION: Answer the question according to the documents. Each line in documents is a row or column extract from table.\\n<</SYS>>\\n\\n\"\\\n",
    "        \"QUESTION: What is the first time this team attend the World Cup?\\n\\n\"\\\n",
    "        \"DOCUMENTS:\\n\"\\\n",
    "        \"Year is 1998, World Cup is No.\\n\"\\\n",
    "        \"Year is 1999, World Cup is No.\\n\"\\\n",
    "        \"Year is 2000, World Cup is Yes.\\n\"\\\n",
    "        \"Year is 2001, World Cup is Yes.\\n\"\\\n",
    "        \"Year is 2002, World Cup is No.\\n\"\\\n",
    "        \"Year is 2003, World Cup is No.\\n\"\\\n",
    "        \"[/INST]\\n\\nResponse:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, stop=[\"[INST]\", \"None\",])\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, stop = [ \"[INST]\"])\n",
    "# sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, )\n",
    "response = model.generate(prompt, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "QUESTION: What is the first time this team attend the World Cup?\n",
      "\n",
      "DOCUMENTS:\n",
      "Year is 1998, World Cup is No.\n",
      "Year is 1999, World Cup is No.\n",
      "Year is 2000, World Cup is Yes.\n",
      "Year is 2001, World Cup is Yes.\n",
      "Year is 2002, World Cup is No\n"
     ]
    }
   ],
   "source": [
    "print(response[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
